\documentclass[letterpaper,12pt]{article}
\usepackage{textcomp}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{gensymb}
\usepackage{enumerate}
\usepackage{fullpage}
\usepackage{setspace}
\onehalfspacing

\setlength{\parindent}{0pt}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand*\conj[1]{\bar{#1}}


\begin{document}

\textbf{\large Problem Set 2}

Natasha Watkins

\vspace{5mm}

\textbf{Exercise 3.1}

\underline{Part i)}

We can write
\begin{equation}
  \label{eq1}
\| \vect{x} + \vect{y} \|^2 = \| \vect{x} \|^2 + \langle \vect{x}, \vect{y} \rangle +
\langle \vect{y}, \vect{x} \rangle + \| \vect{y} \|^2
= \| \vect{x} \|^2 + 2 \langle \vect{x}, \vect{y} \rangle + \| \vect{y} \|^2 
\end{equation}

\begin{math}
\text{as } \langle \vect{y}, \vect{x} \rangle 
= \overline{\langle \vect{x}, \vect{y} \rangle}
= \langle \vect{x}, \vect{y} \rangle \ \text{ in } \mathbb R^n.
\end{math}

Using $\langle \vect{x}, \vect{-y} \rangle = (-1)\langle \vect{x}, \vect{y} \rangle$, we can write

\begin{equation}
  \label{eq2}
\| \vect{x} - \vect{y} \|^2 = \| \vect{x} \|^2 - \langle \vect{x}, \vect{y} \rangle -
\langle \vect{y}, \vect{x} \rangle + \| \vect{y} \|^2
= \| \vect{x} \|^2 - 2 \langle \vect{x}, \vect{y} \rangle + \| \vect{y} \|^2 
\end{equation}

Combining,
\begin{equation*}
  \frac{1}{4} (\| \vect{x} + \vect{y} \|^2 - \| \vect{x} - \vect{y} \|^2)
  = \frac{1}{4}(4 \langle \vect{x}, \vect{y} \rangle) = \langle \vect{x}, \vect{y} \rangle
\end{equation*}

\underline{Part ii)}

Combining equations \ref{eq1} and \ref{eq2}, we find

\begin{equation*}
  \frac{1}{2} (\| \vect{x} + \vect{y} \|^2 + \| \vect{x} - \vect{y} \|^2)
  = \frac{1}{2}(2 \langle \vect{x}, \vect{y} \rangle) = \langle \vect{x}, \vect{y} \rangle
\end{equation*}

\textbf{Exercise 3.2}

\begin{align*}
  \langle \vect{x}, \vect{y} \rangle &= \frac{1}{4}(\|\vect{x} + \vect{y}\|^2 - \|\vect{x} - \vect{y}\|^2 + i\|\vect{x} - i\vect{y}\|^2 - i\|\vect{x} + i\vect{y}\|^2) \\
  &= \frac{1}{4}(\|\vect{x} + \vect{y}\|^2 - \|\vect{x} - \vect{y}\|^2) + \frac{1}{4} (i\|\vect{x} - i\vect{y}\|^2 - i\|\vect{x} + i\vect{y}\|^2) \\
  &= \langle \vect{x}, \vect{y} \rangle + \frac{1}{4} (i\|\vect{x} - i\vect{y}\|^2 - i\|\vect{x} + i\vect{y}\|^2) \\
  &= \langle \vect{x}, \vect{y} \rangle + \frac{1}{4} (i\langle \vect{x} - i\vect{y}, \vect{x} - i\vect{y} \rangle - i\langle \vect{x} + i\vect{y}, \vect{x} + i\vect{y} \rangle) \\
  &= \langle \vect{x}, \vect{y} \rangle + \frac{1}{4}
  (i \langle \vect{x}, \vect{x} - i\vect{y} \rangle +  i^2 \langle \vect{y}, \vect{x} - i\vect{y} \rangle - i \langle \vect{x}, \vect{x} + i\vect{y} \rangle + (-i)^2 \langle \vect{y}, \vect{x} + i\vect{y} \rangle) \\
\begin{split}
    &= \langle \vect{x}, \vect{y} \rangle + \frac{1}{4}
    (i \langle \vect{x}, \vect{x} \rangle + i^2 \langle \vect{x}, \vect{y} \rangle + i^2 \langle \vect{y}, \vect{x} \rangle + i^3 \langle \vect{y}, \vect{y} \rangle \\
    &\qquad - i \langle \vect{x}, \vect{x} \rangle + (-i)^2 \langle \vect{x}, \vect{y} \rangle + (-i)^2 \langle \vect{y}, \vect{x} \rangle + (-i^3) \langle \vect{y}, \vect{y} \rangle)\ \\
\end{split}\\
  &= \langle \vect{x}, \vect{y} \rangle
\end{align*}

\textbf{Exercise 3.3}

\underline{Part i)}

\begin{align*}
  \langle \vect{x}, \vect{x}^5 \rangle = \int_0^1 x^6 dx = \frac{x^7}{7} \Big|_0^1 = \frac{1}{7} \\
  \langle \vect{x}, \vect{x} \rangle = \int_0^1 x^2 dx = \frac{x^3}{3} \Big|_0^1 = \frac{1}{3} 
  \implies \|\vect{x}\| = \sqrt{\frac{1}{3}} \\
  \langle \vect{x}^5, \vect{x}^5 \rangle = \int_0^1 x^{10} dx = \frac{x^{11}}{11} \Big|_0^{1} = \frac{1}{11} \implies \|\vect{x}^5\| = \sqrt{\frac{1}{11}} \\
  \cos(\theta) = \frac{\sqrt{3} \sqrt{11}}{7} = \frac{\sqrt{33}}{7}
  \implies \theta \approx 35 \degree
\end{align*}
  
\underline{Part ii)}

\begin{align*}
  \langle \vect{x}^2, \vect{x}^4 \rangle = \int_0^1 x^6 dx = \frac{x^7}{7} \Big|_0^1 = \frac{1}{7} \\
  \langle \vect{x}^2, \vect{x}^2 \rangle = \int_0^1 x^4 dx = \frac{x^5}{5} \Big|_0^1 = \frac{1}{5} 
  \implies \|\vect{x}^2\| = \sqrt{\frac{1}{5}} \\
  \langle \vect{x}^4, \vect{x}^4 \rangle = \int_0^1 x^{8} dx = \frac{x^{9}}{9} \Big|_0^{1} = \frac{1}{9} \implies \|\vect{x}^4\| = \sqrt{\frac{1}{9}} \\  
  \cos(\theta) = \frac{\sqrt{9} \sqrt{5}}{7} = \frac{\sqrt{45}}{7}
  \implies \theta \approx 17 \degree
\end{align*}

\textbf{Exercise 3.8}

i)

\begin{align*}
\frac{1}{\pi} \int_{-\pi}^{\pi} \cos(t) \sin(t) dt &= 0    \\
\frac{1}{\pi} \int_{-\pi}^{\pi} \cos(t) \cos(2t) dt &= 0   \\
\frac{1}{\pi} \int_{-\pi}^{\pi} \cos(t) \sin(2t) dt &= 0   \\
\frac{1}{\pi} \int_{-\pi}^{\pi} \cos(2t) \sin(t) dt &= 0   \\
\frac{1}{\pi} \int_{-\pi}^{\pi} \cos(t) \cos(t) dt &= 1    \\
\frac{1}{\pi} \int_{-\pi}^{\pi} \sin(t) \sin(t) dt &= 1    \\
\frac{1}{\pi} \int_{-\pi}^{\pi} \sin(2t) \sin(2t) dt &= 1  \\
\end{align*}
Therefore, $S$ is an orthonormal set.

        
ii)
\begin{math}  
\|t\| = \sqrt{\langle t, t \rangle} = \int_{-\pi}^{\pi} t^2 dt
= \frac{t^3}{3} \Big|^\pi_{-\pi} = \frac{2}{3}\pi^2
\end{math}

iii)
\begin{math}
  \text{proj}_X(\cos(3t)) = \sum_{i=1}^m \langle \vect{x}_i, \cos(3t) \rangle 
  \vect{x_i} = 0
\end{math}

iv)
\begin{math}
  \text{proj}_X(t) = \sum_{i=1}^m \langle \vect{x}_i, t \rangle 
  \vect{x_i} = 1
\end{math}

\textbf{Exercise 3.9}

By Theorem 3.2.15, a matrix $Q$ is orthonormal if and only if $Q^HQ = QQ^H = 1$.

The rotation matrix is given by
\begin{align*}
  R_\theta = 
  \begin{bmatrix}
    \cos \theta & -\sin \theta \\
    \sin \theta & \cos \theta \\
  \end{bmatrix}
\end{align*}

Calculating $R_{\theta}R$, we find
\begin{align*}
  R_{\theta}R = 
  \begin{bmatrix}
    \cos \theta & -\sin \theta \\
    \sin \theta & \cos \theta \\
  \end{bmatrix}
  &=
  \begin{bmatrix}
    \cos \theta & \sin \theta \\
    -\sin \theta & \cos \theta \\
  \end{bmatrix}
  \begin{bmatrix}
    \cos \theta & -\sin \theta \\
    \sin \theta & \cos \theta \\
  \end{bmatrix}
  \\ &=
  \begin{bmatrix}
    \cos^2 \theta + \sin^2 \theta & \cos \theta \sin \theta - \sin \theta \cos \theta \\
    \sin \theta \cos \theta - \cos \theta \sin \theta & \sin^2 \theta + \cos^2 \theta \\
  \end{bmatrix}
  \\ &=
  \begin{bmatrix}
    1 & 0 \\
    0 & 1 \\
  \end{bmatrix}
  = I
\end{align*}
 
So $R_\theta$ is an orthonormal transformation.

\textbf{Exercise 3.10}

\underline{Part i)}

Assume $Q$ is orthonormal, which implies $\langle \vect{e}_i, \vect{e}_j \rangle = \langle Q \vect{e}_i,  Q \vect{e}_j \rangle$.
\begin{align*}
  \vect{e}_i^H \vect{e}_j &= 
  \langle \vect{e}_i, \vect{e}_j \rangle \\
  &= \langle Q \vect{e}_i,  Q \vect{e}_j \rangle \\
  &= (Q \vect{e}_i)^H  (Q \vect{e}_j) \\
  &= \vect{e}_i^H Q^H  Q \vect{e}_j
\end{align*}

$\vect{e}_i^H \vect{e}_j = \vect{e}_i^H Q^H  Q \vect{e}_j$ only if $Q^HQ = I$.

\underline{Part ii)}

\begin{align*}
  \| Q \vect{x} \| &= \sqrt{\langle Q \vect{x}, Q \vect{x} \rangle} \\
  &= \sqrt{\vect{x}^H Q^H Q \vect{x}} \\
  &= \sqrt{\vect{x}^H \vect{x}} \quad \quad \quad \text{as } Q^HQ = 1 \\
  &= \sqrt{\langle \vect{x}, \vect{x} \rangle} \\
  &= \| \vect{x} \|
\end{align*}

\underline{Part iii)}

$Q^HQ = I$ so $Q^H = Q^{-1}$.
\begin{align*}
  \langle Q^{-1} \vect{x}, Q^{-1} \vect{x} \rangle
  &= \vect{x}^H (Q^{-1})^H Q^{-1} \vect{x} \\
  &= \vect{x}^H (Q^H)^H Q^H \vect{x} \quad \quad \text{as } Q^H = Q^{-1} \\
  &= \vect{x}^H Q Q^H \vect{x} \\
  &= \vect{x}^H \vect{x} \quad \quad \text{as } QQ^H = 1 \\
  &= \langle \vect{x}, \vect{x} \rangle
\end{align*}

So $Q^{-1}$ is orthonormal.

\underline{Part iv)}

$Q_{n, m}$ is an orthonormal matrix. We can write $Q$ as

\begin{align*}
  Q =
\begin{bmatrix}
  \vect{q}_1 & \vect{q}_2 & \cdots & \vect{q}_m
\end{bmatrix}
\end{align*}

Where $\vect{q}_i$ is a $(n \times 1)$ column of $Q$, and $\{\vect{q}_i\}_{i=1}^m$ is a set of columns vectors.

As $Q$ is orthonormal, we know $Q^HQ = I_m$. So we can write

\begin{align*}
Q^HQ &=
\begin{bmatrix}
  \vect{q}_1 & \vect{q}_2 & \cdots & \vect{q}_m
\end{bmatrix}^H
\begin{bmatrix}
  \vect{q}_1 & \vect{q}_2 & \cdots & \vect{q}_m
\end{bmatrix} \\ &=
\begin{bmatrix}
  \vect{q}_1^H \\ \vect{q}_2^H \\ \vdots \\ \vect{q}_m^H
\end{bmatrix}
\begin{bmatrix}
  \vect{q}_1 & \vect{q}_2 & \cdots & \vect{q}_m
\end{bmatrix} \\ &=
\begin{bmatrix}
  \vect{q}_1^H \vect{q}_1 & \vect{q}_2^H \vect{q}_1 & \cdots & \vect{q}_m^H \vect{q}_1 \\
  \vect{q}_1^H \vect{q}_2 & \vect{q}_2^H \vect{q}_2 & \cdots & \vdots \\
  \vdots & \vdots & \ddots & \\
  \vect{q}_1^H \vect{q}_2 & \cdots & & \vect{q}_m^H \vect{q}_m
\end{bmatrix} \\ &=
\begin{bmatrix}
  1 & 0 & \cdots & 0 \\
  0 & 1 & \cdots & \vdots \\
  \vdots & \vdots & \ddots & \\
  0 & \cdots & & 1
\end{bmatrix}
\end{align*}
which satisfies the definition of an orthonormal set, ie. $\langle \vect{q}_i, \vect{q}_j \rangle = \delta_{i, j}$ where
\begin{align*}
\delta_{i, j}=
\begin{cases}
1 & \text{if } i = j \\
0 & \text{if } i \neq j \\
\end{cases}
\end{align*}

\underline{Part v)}

It is not true that $|\det(Q)| = 1$ implies Q is orthonormal. For example
\begin{align*}
  Q = 
  \begin{bmatrix}
    2 & \sqrt{3} \\
    \sqrt{3} & 2 \\
  \end{bmatrix}
\end{align*}
has $|\det(Q)| = 1$ but
\begin{align*}
  Q^HQ = 
  \begin{bmatrix}
    2 & \sqrt{3} \\
    \sqrt{3} & 2 \\
  \end{bmatrix}
  \begin{bmatrix}
    2 & \sqrt{3} \\
    \sqrt{3} & 2 \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    1 & 2\sqrt{3} - 3\sqrt{2} \\
    3\sqrt{2} -  2\sqrt{3} & -1 \\
  \end{bmatrix}
  \neq I
\end{align*}
So $Q$ is not orthonormal.

\underline{Part vi)}

$Q_1$ and $Q_2$ are orthonormal, so $Q_1^HQ_1 = I$ and $Q_2^HQ_2 = I$
Given $Q = Q_1 Q_2$
\begin{align*}
  Q^HQ &= (Q_1Q_2)^HQ_1Q_2 \\
  &= Q_2^H Q_1^H Q_1 Q_2 \\
  &= Q_2^H Q_2 \\
  &= I
\end{align*}
$Q$ is therefore orthonormal.

\textbf{Exercise 3.11}

If the Gramâ€“--Schmidt orthonormalisation process is applied to a collection of linearly dependent vectors, it outputs the $\vect{0}$ vector for some $\vect{x}_k - \vect{p}_{k-1}$, and therefore $\vect{q}_k$ is undefined. 

\textbf{Exercise 3.16}

\underline{Part i)}

Consider a diagonal matrix $D$ with elements on the diagonal equal to $-1$. We can then write $Q^* = QD$ and $R^* = D^{-1}R$, and deduce that $A = Q^* R^* = QDD^{-1}R = QR$, showing that the QR decomposition is not unique.

\underline{Part ii)} 

Suppose $A = Q_1 R_1$ and $A = Q_2 R_2$, , where $R_1, R_2$ are upper triangular matrices with positive elements along the diagonal.

Then $Q_1 R_1 = Q_2 R_2 \implies Q_2^{-1} Q_1 = R_1^{-1}R_2$, which means that $R_1^{-1}R_2$ is orthonormal. As $R_1^{-1}R_2$ is an upper triangular matrix that is orthonormal, its diagonal elements must be equal to 1 (as the diagonal of $R$ is positive). So $Q_2^{-1} Q_1 = I$ and $R_1^{-1}R_2 = I$, and thus $Q_1 = Q_2$ and $R_1 = R_2$, showing that the decomposition is unique in this case.

\textbf{Exercise 3.17}
\begin{align*}
  A^H A \vect{x} &= (\hat{Q} \hat{R})^H \hat{Q} \hat{R} \vect{x} \\
  &= \hat{R}^H \hat{Q}^H \hat{Q} \hat{R} \vect{x} \\
  &= \hat{R}^H \hat{R} \vect{x} \quad \quad \text{as }  \hat{Q}^H \hat{Q} = 1\\
  \hat{R}^H \hat{R} \vect{x} &= \hat{R}^H \hat{Q}^H \vect{b} \quad \quad \text{as } A^H = \hat{R}^H \hat{Q}^H \\
  \hat{R} \vect{x} &= \hat{Q}^H \vect{b} \quad \quad \text{ as required }
\end{align*}

\textbf{Exercise 3.23}

By the triangle inequality,
\begin{equation*}
  \|\vect{x}\| = \|(\vect{x} - \vect{y}) + \vect{y}\| \leq \|\vect{x} - \vect{y}\| + \|\vect{y}\|
  \Leftrightarrow \|\vect{x}\| - \|\vect{y}\| \leq \|\vect{x} - \vect{y}\|
\end{equation*}

By scale preservation,
\begin{equation*}
  \|\vect{y}\| = \|-1\| \cdot \|\vect{y}\| = \|-\vect{y}\| = \|\vect{x} - \vect{y} + \vect{x}\| \leq \|\vect{x} - \vect{y}\| + \|\vect{y}\|
  \Leftrightarrow \|\vect{y}\| - \|\vect{x}\| \leq \|\vect{x} - \vect{y}\|
\end{equation*}

\textbf{Exercise 3.24}

\underline{Part i)}

\begin{enumerate}
  \item \textbf{Positivity:}
  
        Consider $f, g \in C[a, b]$, with $f(t) \geq g(t)$ for all $t \in [a, b]$, then
        \begin{align*}
          \int_a^b f(t)dt \geq \int_a^b g(t)dt 
        \end{align*}
        With $g(t) = 0$, and $|f(t)| \geq 0$
        \begin{align*}
          \int_a^b f(t)dt \geq \int_a^b 0 dt = 0
        \end{align*}
  \item \textbf{Scale preservation:}
  
        Given $h \in \mathbb F$ and $f \in C[a, b]$,
        \begin{align*}
          \| hf \| &= \int_a^b |h f(t)| dt = \int_a^b |h||f(t)| dt =
          |h| \int_a^b |f(t)| dt = |h| \|f\|
        \end{align*}
  \item \textbf{Triangle inequality:}
        
        Consider $f, g \in C[a, b]$.
        \begin{align*}
          \| f + g \| &= \int_a^b |f(t) + g(t)| dt \\
          &\leq \int_a^b (|f(t)| + |g(t)|) dt \\
          &= \int_a^b |f(t)|dt + \int_a^b |g(t)|dt \\
          &= \|f\| + \|g\| 
        \end{align*}
\end{enumerate}

\underline{Part ii)}

\begin{enumerate}
	\item \textbf{Positivity: } $|f(t)|^2 \geq 0$ for all $x which implies (\int_a^b |f(t)|^2dt)^\frac{1}{2} \geq 0$. Also, $(\int_a^b |f(t)|^2 dt)^\frac{1}{2} = 0$ if and only if $|f(t)| = 0$.
	\item \textbf{Scale preservation: }$||af||_{L^2} = (\int_a^b |af(t)|^2 dt)^\frac{1}{2} = (|a|^2 \int_a^b |f(t)|^2 dt)^\frac{1}{2} = |a| ||f||_{L^2}$.
	\item \textbf{Triangle inequality: } Consider $f, g \in C[a, b]$.
		\begin{align*}
		|| f + g || &= \Big(\int_a^b |f(t) + g(t)|^2 dt\Big)^\frac{1}{2} \\
		&\leq \Big(\int_a^b |f(t)|^2dt + \int_a^b |g(t)|^2dt\Big)^\frac{1}{2} \\
		&\leq ||f||_{L^2} + ||g||_{L^2} \quad \quad \text{by Cauchy-Schwarz}
		\end{align*}
\end{enumerate}




\underline{Part iii)}

\begin{enumerate}
  \item \textbf{Positivity:}
  
        $|f(x)| \geq 0$ for all $x$, so it follows that $\sup_{x \in [a, b]} |f(x)| \geq 0$.
  \item \textbf{Scale preservation:}
        \begin{align*}
          \|hf\| = \sup_{x \in [a, b]} |h||f(x)| = |h| \sup_{x \in [a, b]} |f(x)| = |h|\|f\|
        \end{align*}
  \item \textbf{Triangle inequality:}
  
        For any $x \in [a, b]$
        \begin{align*}
          |f(x) + g(x)| &\leq |f(x)| + |g(x)| \\
                        &\leq \sup_{x \in [a, b]} |f(x)| + \sup_{x \in [a, b]} |g(x)| \\
                        &= \|f\| + \|g\|
        \end{align*}
        As this holds for all $x \in [a, b]$,
        \begin{align*}
          \sup_{x \in [a, b]} |f(x) + g(x)| = \|f + g \| \leq \|f\| + \|g\|
        \end{align*}
\end{enumerate}

\textbf{Exercise 3.26}

\underline{$\| \vect{x} \|_a \sim \| \vect{x} \|_a$}
\begin{align*}
  & m \| \vect{x} \|_a \leq \|\vect{x}\|_a \leq M \| \vect{x} \|_a
  \\
  & \implies m \leq 1 \leq M
\end{align*}
with $m \in [0, 1]$, and $M \in [1, \infty]$.

\underline{$ \| \vect{x} \|_a \sim \| \vect{x} \|_b \implies \| \vect{x} \|_b \sim \| \vect{x} \|_a$}
\begin{align*}
  & m \| \vect{x} \|_a \leq \|\vect{x}\|_b \leq M \| \vect{x} \|_a
  \\
  & \implies \frac{1}{m} \leq \frac{\|\vect{x}\|_b}{\| \vect{x} \|_a } \leq \frac{1}{M} \\
  & \implies \frac{1}{M} \geq \frac{\|\vect{x}\|_a}{\| \vect{x} \|_b } \geq \frac{1}{m} \\
  & \implies Y \geq \frac{\|\vect{x}\|_a}{\| \vect{x} \|_b } \geq y \\
  & \implies y \| \vect{x} \|_b  \leq \|\vect{x}\|_a \leq Y \| \vect{x} \|_b 
\end{align*}
where $0 \leq y \leq Y$.

\underline{$ \| \vect{x} \|_a \sim \| \vect{x} \|_b \text{ and } \| \vect{x} \|_b \sim \| \vect{x} \|_c  \implies \| \vect{x} \|_a \sim \| \vect{x} \|_c$}
\begin{align}
  \label{ineq1}
  & m \| \vect{x} \|_a \leq \|\vect{x}\|_b \leq M \| \vect{x} \|_a
\\
  \label{ineq2}
  & w \| \vect{x} \|_b \leq \|\vect{x}\|_c \leq W \| \vect{x} \|_b
\end{align}
Multiply (\ref{ineq1}) by $w$, to get
\begin{align}
  \label{ineq1_w}
  & mw \| \vect{x} \|_a \leq w\|\vect{x}\|_b \leq wM \| \vect{x} \|_a
\end{align}
Combining (\ref{ineq1_w}) with (\ref{ineq2}), we see
\begin{align*}
  & mw \| \vect{x} \|_a \leq \|\vect{x}\|_c \leq W \| \vect{x} \|_b
\end{align*}
Similarily,
\begin{align*}
  & mw \| \vect{x} \|_a \leq \|\vect{x}\|_c \leq MW \| \vect{x} \|_a
\end{align*}
where $0 \leq mw \leq MW$, as required.

\underline{Part i)}
\begin{align*}
  \| \vect{x} \|_2 
  &= (|\vect{x}_1|^2 + |\vect{x}_2|^2 + \cdots + |\vect{x}_n|^2)^{1/2} \\
  & \leq |\vect{x}_1| + |\vect{x}_2| + \cdots + |\vect{x}_n| \quad \quad \text{ by Cauchy-Schwarz} \\
  &= \| \vect{x} \|_1
\end{align*}

\textbf{Exercise 3.29}

The induced norm on $M_n(\mathbb F)$ is given by
$$
\| A \|_2 = \sup \frac{\|A \vect{x} \|_2}{\| \vect{x} \|_2}
$$
Given an orthonormal matrix $Q$,
\begin{align*}
  \| Q \|_2 &= \sup \frac{\|Q \vect{x} \|_2}{\| \vect{x} \|_2} \\
  &= \sup \frac{\|\vect{x} \|_2}{\| \vect{x} \|_2} \\
  &= 1
\end{align*}
Given a matrix norm, we know $\|A \vect{x} \| \leq \|A \| \|\vect{x} \|$. So
\begin{align*}
  \|R_{\vect{x}}\| &= \sup \frac{\| A\vect{x} \|}{\|A\|} \\
  &= \sup \frac{\| A\vect{x} \| \| \vect{x} \| }{\|A\| \| \vect{x} \|} \\
  &\leq \sup \frac{\| A\vect{x} \| \| \vect{x} \| }{\|A \vect{x} \|} \\
  &= \| \vect{x} \|
\end{align*}
Consider $\vect{x}$ such that $\|\vect{x}\| = 1$.
If $\|R_{\vect{x}}\| < \|\vect{x}\| = 1$, it must be that $\|A\|\|\vect{x}\| < \|A \vect{x} \|$. This can be written as
\begin{align*}
  \| A \| < \frac{\|A \vect{x}\|}{\|\vect{x}\|}
\end{align*}

\textbf{Exercise 3.30}

$\|A\|$ is a matrix norm. $\|A\|_s = \|SAS^{-1}\|$.
\begin{enumerate}
	\item \textbf{Positivity: } $\|A\| \geq 0 $, so $\|SAS^{-1}\| = \|A\|_s \geq 0$
	\item \textbf{Scale preservation: }
		\begin{align*}
		\|\alpha A\|_S = \|S \alpha AS^{-1}\| = \sup \frac{\| S\alpha A(\vect{x})S^{-1} \|}{\|\vect{x}\|} & \leq \sup \frac{|\alpha| \|SA(\vect{x})S^{-1} \|}{\|\vect{x}\|} \\ &= |\alpha| \sup \frac{\|SA(\vect{x})S^{-1} \|}{\|\vect{x}\|} \\ &= |\alpha| \|S \alpha AS^{-1}\| \\ &= |\alpha| \|A\|_S
		\end{align*}
	\item \textbf{Triangle inequality: }
		\begin{align*}
		\|A + B \|_S &= \|S (A + B) S^{-1}\| \\
		&= \|S A S^{-1} + S B S^{-1}\| \\
		&= \sup \frac{\|S A(\vect{x}) S^{-1} + S B(\vect{x}) S^{-1}\|}{\| \vect{x} \|} \\
		&\leq \sup \frac{\|S A(\vect{x}) S^{-1}\| + \| S B(\vect{x}) S^{-1}\|}{\| \vect{x} \|} \\
		&= \|SAS^{-1}\| + \|SBS^{-1}\| \\
		&= \|A\|_S + \|B\|_S
		\end{align*}
\end{enumerate}

So $\|\cdot\|_S$ is also a matrix norm.

\textbf{Exercise 3.40}

\underline{Part i)}
\begin{align*}
\langle L(C), B \rangle
&= \langle AC, B \rangle \\
&= tr((AC)^HB) \\
&= tr(C^H A^H B) \\
&= \langle C, A^H B \rangle \\
&= \langle C, L(B) \rangle \quad \text{with } A^* = A^H
\end{align*}

\underline{Part ii)}
\begin{align*}
\langle A_2, A_3 A_1 \rangle
&= tr(A_2^H A_3 A_1) \\
&= tr((A_2^H A_3 A_1)^H) \\
&= tr(A_1^HA_3^H A_2) \\
&= tr(A_1^*A_3^H A_2) \\
&= tr(A_3^H A_1^* A_2) \\
&= \langle A_3, A_1^* A_2 \rangle
\end{align*}

\textbf{Exercise 3.47}

$P = A(A^HA)^{-1}A^H$.

\begin{enumerate}[i.]
  \item $P^2 = A(A^HA)^{-1}A^H A(A^HA)^{-1}A^H$
  \item $P^H = (A(A^HA)^{-1}A^H)^H = A ((A^HA)^{-1})^H A^H
        = A (A^HA)^H)^{-1} A^H = A (A^HA)^{-1} A^H = P$
  \item $P$ is an idempotent matrix by i), which means $rank(P) =     tr(P)$.
        $$
        tr(P) = tr(A(A^HA)^{-1}A^H) = tr(A^HA(A^HA)^{-1}) = tr(I_n) = n = rank(P)
        $$
        
\end{enumerate}

\end{document}